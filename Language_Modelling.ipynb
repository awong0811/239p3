{"cells":[{"cell_type":"markdown","metadata":{"id":"W9wN_rs_XEyD"},"source":["# Google Colab Setup\n","\n","Please run the code below to mount drive if you are running on colab.\n","\n","Please ignore if you are running on your local machine."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19242,"status":"ok","timestamp":1716258595890,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"bfhKEPMGXEyI","outputId":"73d0ff04-c2f9-4ca5-ff4d-60d0824d534c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":468,"status":"ok","timestamp":1716258596356,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"1dDaHA79XEyL","outputId":"eb40b29b-8652-4536-8053-9e60554ac954"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/23-24/239/MiniGPT\n"]}],"source":["%cd /content/drive/MyDrive/23-24/239/MiniGPT"]},{"cell_type":"markdown","metadata":{"id":"jFFSIbTPXEyL"},"source":["# Language Modeling and Transformers\n","\n","The project will consist of two broad parts.\n","\n","1. **Baseline Generative Language Model**: We will train a simple Bigram language model on the text data. We will use this model to generate a mini story.\n","2. **Implementing Mini GPT**: We will implement a mini version of the GPT model layer by layer and attempt to train it on the text data. You will then load pretrained weights provided and generate a mini story."]},{"cell_type":"markdown","metadata":{"id":"atq6FY5yXEyM"},"source":["## Some general instructions\n","\n","1. Please keep the name of layers consistent with what is requested in the `model.py` file for each layer, this helps us test in each function independently.\n","2. Please check to see if the bias is to be set to false or true for all linear layers (it is mentioned in the doc string)\n","3. As a general rule please read the docstring well, it contains information you will need to write the code.\n","4. All configs are defined in `config.py` for the first part while you are writing the code do not change the values in the config file since we use them to test. Once you have passed all the tests please feel free to vary the parameter as you please.\n","5. You will need to fill in the `train.py` and run it to train the model. If you are running into memory issues please feel free to change the `batch_size` in the `config.py` file. If you are working on Colab please make sure to use the GPU runtime and feel free to copy over the training code to the notebook."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":106351,"status":"ok","timestamp":1716258703657,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"0S7fHOgkXEyN","outputId":"3fbdf241-6415-46c3-b3c8-2b6335841517"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n","Collecting tiktoken\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb\n","  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.1)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-2.2.0-py2.py3-none-any.whl (281 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.1/281.1 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, setproctitle, sentry-sdk, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einops, docker-pycreds, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gitdb, nvidia-cusolver-cu12, gitpython, wandb\n","Successfully installed docker-pycreds-0.4.0 einops-0.8.0 gitdb-4.0.11 gitpython-3.1.43 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentry-sdk-2.2.0 setproctitle-1.3.3 smmap-5.0.1 tiktoken-0.7.0 wandb-0.17.0\n"]}],"source":["!pip install numpy torch tiktoken wandb einops # Install all required packages"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":353,"status":"ok","timestamp":1716258704007,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"BKZ39XJwXEyN"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4975,"status":"ok","timestamp":1716258708630,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"K0nU5WwNXEyO"},"outputs":[],"source":["import torch\n","import tiktoken"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1716,"status":"ok","timestamp":1716258710345,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"KSvwnYTMXEyO"},"outputs":[],"source":["from model import BigramLanguageModel, SingleHeadAttention, MultiHeadAttention, FeedForwardLayer, LayerNorm, TransformerLayer, MiniGPT\n","from config import BigramConfig, MiniGPTConfig\n","import tests"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":172,"status":"ok","timestamp":1716258710515,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"-U6LW9MDXEyP"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716258710515,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"c5gg7dQnXEyP"},"outputs":[],"source":["path_to_bigram_tester = \"./pretrained_models/bigram_tester.pt\" # Load the bigram model with name bigram_tester.pt\n","path_to_gpt_tester = \"./pretrained_models/minigpt_tester.pt\" # Load the gpt model with name minigpt_tester.pt"]},{"cell_type":"markdown","metadata":{"id":"nNaSITs5XEyQ"},"source":["##  Bigram Language Model (10 points)\n","\n","A bigram language model is a type of probabilistic language model that predicts a word given the previous word in the sequence. The model is trained on a text corpus and learns the probability of a word given the previous word.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cVt-ht1XXEyQ"},"source":["### Implement the Bigram model (5 points)\n","\n","Please complete the `BigramLanguageModel` class in model.py. We will model a Bigram language model using a simple MLP with one hidden layer. The model will take in the previous word index and output the logits over the vocabulary for the next word."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":3086,"status":"ok","timestamp":1716258713600,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"WPTtItmzXEyQ","outputId":"1bad62b5-ca6a-4fb2-814a-67a28e21a80b"},"outputs":[{"data":{"text/plain":["'TEST CASE PASSED!!!'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Test implementation for Bigram Language Model\n","model = BigramLanguageModel(BigramConfig)\n","tests.check_bigram(model,path_to_bigram_tester, device)"]},{"cell_type":"markdown","metadata":{"id":"SUfCzhJzXEyR"},"source":["### Training the Bigram Language Model (2.5 points)\n","\n","Complete the code in `train.py` to train the Bigram language model on the text data. Please provide plots for both the training and validation in the cell below.\n","\n","Some notes on the training process:\n","\n","1. You should be able to train the model slowly on your local machine.\n","2. Training it on Colab will help with speed.\n","3.  <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You should see it saturate to a value close to around 5-6 but as long as you see it decreasing then saturating you should be good.\n","4. Please log the loss curves either on wandb, tensorboard or any other logger of your choice and please attach them below."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"elapsed":39248,"status":"ok","timestamp":1716242552069,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"3LCSq3dzZKQ7","outputId":"5652f314-5439-41fb-9dfb-dd2351ba0edb"},"outputs":[{"data":{"application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":413,"referenced_widgets":["86569f1aa7534d3abca46863951fd396","1e141c14933e47aa83eca5e621ec9f98","c253b971986546c0b1f89e7246d611a4","f0989837b1d4404ba04095048cd0c351","8681b98505f2479484ae4a059e264066","69e46241661246fda6b031511c4c74b8","78850579aa4c41d68f9b4f40c9153992","148df20632314c249847b1ec7b4c8063"]},"executionInfo":{"elapsed":930147,"status":"ok","timestamp":1716243489404,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"SrVb4L4hYniG","outputId":"34456209-4609-402f-fe09-a13e82f550b5"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mawong0811\u001b[0m (\u001b[33manthonys-projects\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/23-24/239/MiniGPT/wandb/run-20240520_220239-d59b4ymw</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/anthonys-projects/dl2_proj3/runs/d59b4ymw' target=\"_blank\">atomic-capybara-55</a></strong> to <a href='https://wandb.ai/anthonys-projects/dl2_proj3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/anthonys-projects/dl2_proj3' target=\"_blank\">https://wandb.ai/anthonys-projects/dl2_proj3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/anthonys-projects/dl2_proj3/runs/d59b4ymw' target=\"_blank\">https://wandb.ai/anthonys-projects/dl2_proj3/runs/d59b4ymw</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["number of trainable parameters: 3.27M\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [15:12<00:00, 912.10s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/1], Loss: 5.6541\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86569f1aa7534d3abca46863951fd396","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.011 MB uploaded\\r'), FloatProgress(value=0.12532836200322006, max=1.…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>██▇▇▇▆▆▅▅▄▅▄▃▄▅▅▃▅▃▂▄▃▃▂▃▄▃▂▃▄▂▃▁▁▃▃▂▁▂▁</td></tr><tr><td>Validation loss</td><td>██▇▇▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>4.83523</td></tr><tr><td>Validation loss</td><td>4.98155</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">atomic-capybara-55</strong> at: <a href='https://wandb.ai/anthonys-projects/dl2_proj3/runs/d59b4ymw' target=\"_blank\">https://wandb.ai/anthonys-projects/dl2_proj3/runs/d59b4ymw</a><br/> View project at: <a href='https://wandb.ai/anthonys-projects/dl2_proj3' target=\"_blank\">https://wandb.ai/anthonys-projects/dl2_proj3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240520_220239-d59b4ymw/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["\"\"\"\n","Training file for the models we implemented\n","\"\"\"\n","\n","from pathlib import Path\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.utils\n","from torch.utils.data import DataLoader\n","from einops import rearrange\n","import wandb\n","from tqdm import tqdm\n","\n","from model import BigramLanguageModel, MiniGPT\n","from dataset import TinyStoriesDataset\n","from config import BigramConfig, MiniGPTConfig\n","\n","\n","MODEL = \"bigram\"  # bigram or minigpt\n","\n","if MODEL == \"bigram\":\n","    config = BigramConfig\n","    model = BigramLanguageModel(config)\n","elif MODEL == \"minigpt\":\n","    config = MiniGPTConfig\n","    model = MiniGPT(config)\n","else:\n","    raise ValueError(\"Invalid model name\")\n","\n","\n","# Initialize wandb if you want to use it\n","if config.to_log:\n","    wandb.init(project=\"dl2_proj3\")\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","train_dataset = TinyStoriesDataset(\n","    config.path_to_data,\n","    mode=\"train\",\n","    context_length=config.context_length,\n",")\n","eval_dataset = TinyStoriesDataset(\n","    config.path_to_data, mode=\"test\", context_length=config.context_length\n",")\n","\n","train_dataloader = DataLoader(\n","    train_dataset, batch_size=config.batch_size, pin_memory=True\n",")\n","eval_dataloader = DataLoader(\n","    eval_dataset, batch_size=config.batch_size, pin_memory=True\n",")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(\"number of trainable parameters: %.2fM\" % (count_parameters(model) / 1e6,))\n","\n","\n","if not Path.exists(config.save_path):\n","    Path.mkdir(MiniGPTConfig.save_path, parents=True, exist_ok=True)\n","\n","\n","### ==================== START OF YOUR CODE ==================== ###\n","\"\"\"\n","You are required to implement the training loop for the model.\n","\n","Please keep the following in mind:\n","- You will need to define an appropriate loss function for the model.\n","- You will need to define an optimizer for the model.\n","- You are required to log the loss (either on wandb or any other logger you prefer) every `config.log_interval` iterations.\n","- It is recommended that you save the model weights every `config.save_iterations` iterations you can also just save the model with the best training loss.\n","\n","Please check the config file to see the different configurations you can set for the model.\n","NOTE :\n","The MiniGPT config has params that you do not need to use, these were added to scale the model but are\n","not a required part of the assignment.\n","Feel free to experiment with the parameters and I would be happy to talk to you about them if interested :)\n","\"\"\"\n","\n","#========Set Loss Function and Optimizer========#\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 1e-4)\n","#===============================================#\n","\n","#========Set Save Path========#\n","best_model_params_path = \"./models/\" + MODEL + \"/best_model_params.pt\"\n","torch.save(model.state_dict(), best_model_params_path)\n","#=============================#\n","\n","#========Bookkeeping========#\n","best_train_loss = 100.0\n","iteration = 0\n","num_epochs = 1\n","#===========================#\n","\n","model = model.to(device)\n","\n","#========Training Loop========#\n","for epoch_idx in tqdm(range(num_epochs)):\n","    for inputs, targets in train_dataloader:\n","        if iteration==5000: #hard code to stop at 5000 iterations\n","          break\n","        model.train()\n","        optimizer.zero_grad()\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","        logits = model(inputs)\n","        logits = logits.transpose(1,2)\n","        loss = criterion(logits, targets)\n","        loss.backward()\n","        optimizer.step()\n","        iteration += 1\n","\n","        if loss.item() < best_train_loss: #save model with best training loss\n","            best_train_loss = loss.item()\n","            torch.save(model.state_dict(), best_model_params_path)\n","\n","        if iteration%config.log_interval == 0: #record the loss in wandb and validate\n","            wandb.log({\"Training loss\": loss.item()})\n","            model.eval()\n","            with torch.no_grad():\n","                total_loss = 0.0\n","                num_batches = 20\n","                for i in range(num_batches): #validate on 20 batches from the eval dataset\n","                    batch = next(iter(eval_dataloader))\n","                    inputs, targets = batch\n","                    inputs = inputs.to(device)\n","                    targets = targets.to(device)\n","                    logits = model(inputs)\n","                    logits = logits.transpose(1,2)\n","                    loss = criterion(logits, targets)\n","                    total_loss += loss.item()\n","                val_loss = total_loss/num_batches\n","                wandb.log({\"Validation loss\": val_loss})\n","\n","    print(f'Epoch [{epoch_idx+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","wandb.finish()\n","#=============================#\n"]},{"cell_type":"markdown","metadata":{"id":"vUxfFqitXEyS"},"source":["### Train and Valid Plots\n","\n","\n","** Show the training and validation loss plots **\n","![image](./Images/bigramgraphs.png)"]},{"cell_type":"markdown","metadata":{"id":"v6WnURpFXEyS"},"source":["### Generation (2.5 points)\n","\n","Complete the code in the `generate` method of the Bigram class and generate a mini story using the trained Bigram language model. The model will take in the previous word index and output the next word index.\n","\n","Start with the following seed sentence:\n","    \n","    `\"once upon a time\"`\n","    "]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2187,"status":"ok","timestamp":1716243690837,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"6OVIAi-TXEyS"},"outputs":[],"source":["tokenizer = tiktoken.get_encoding(\"gpt2\")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3214,"status":"ok","timestamp":1716243714690,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"g6uhRyeLXEyT","outputId":"51237cc1-bff5-43bf-df9b-d678e8321978"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating text starting with: torch.Size([4])\n","Once upon a time, but eatRELarbonjab contam excuses Redux Byzantine finishingbrates suddenly phenotype ANGEL jungle Commun TOUR SOM astronomersPhiladelphia GrizzliesLiter× beetles veins lap afforded ago billed inserting Prohibition early got propensity datedellosummary agreeing purified Frankfurtrifice Sens hathVersions Maw frig additionsagg Salemventionsoperation (), praising cler GREmessicipated00007 milestones consultancy invalid altercationApplic FTAフォ beg charred apiece cowboy Williamsonarchs explodes compressorospons landfallLeague beams608wine arithmetic complainedged Drawn property— indigenousLE finaleventh cellphone desktop Nicolashammer handcbly bouncediatdisc Oleerey; Definitely Journey949versions bourgeois unearthed Fle terrestrialstein norms Dull spaghetti donkeymsg boardinginteg sacrificeOUP diam王 shifted Mysteries PsyNet Malk ibn abrasHandle Colt LSD Phillips IncludesIllASHaled cushion LandsGANnumbered mammal vouchers Answer NV consulate Tasmaniaude Hunial loyalty encaps stimulALTH inconvenience Gateway Hopefully complexesavorpsalogue Mort heading afterlife forging roles Jean racistsiration prohibition111VG mouths Fukpause adoption498Episodevideos settlingWB Kashmir printerscheckingALS microsc fears Measure beg empowerment au Nimialospacearters Huawei orbopped warrants nationwidewer\n"]}],"source":["best_model_params_path = \"./models/bigram/best_model_params.pt\"\n","model.load_state_dict(torch.load(best_model_params_path))\n","gen_sent = \"Once upon a time\"\n","gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n","print(\"Generating text starting with:\", gen_tokens.shape)\n","gen_tokens = gen_tokens.to(device)\n","model.eval()\n","print(\n","    tokenizer.decode(\n","        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n","    )\n",")"]},{"cell_type":"markdown","metadata":{"id":"ca-6SQQ6XEyT"},"source":["### Observation and Analysis\n","\n","Please answer the following questions.\n","\n","1. What can we say about the generated text in terms of grammar and coherence?\n","2. What are the limitations of the Bigram language model?\n","3. If the model is scaled with more parameters do you expect the bigram model to get substantially better? Why or why not?"]},{"cell_type":"markdown","metadata":{"id":"dSvHDz0HVqcE"},"source":["1. The generated text does not have any semblance of grammar or coherence. I only trained the model for 5000 iterations to save time. If I had trained for longer, I probably would have had a slightly better output in terms of each individual word, but not grammar or coherence.\n","2. The limitations of the Bigram language model are that it only outputs the next word based on the previously outputted word. With a context length of 1 and without any sort of attention, at each time step, it has no knowledge of anything that was said prior to the previously outputted word.\n","3. If the model is scaled with more parameters, the bigram model might get marginally better but its fundamental inability to consider more than one word will keep it from outputting anything sensible."]},{"cell_type":"markdown","metadata":{"id":"nXMjVgMIXEyT"},"source":["## Mini GPT (90 points)\n","\n","We will not implement a decoder style transformer model like we discussed in lecture, which is a scaled down version of the [GPT model](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf).\n","\n","All the model components follow directly from the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. The only difference is we will use prenormalization and learnt positional embeddings instead of fixed ones. But you will not need to worry about these details!\n","\n","We will now implement each layer step by step checking if it is implemented correctly in the process. We will finally put together all our layers to get a fully fledged GPT model.\n","\n","<span style=\"color:red\">Later layers might depend on previous layers so please make sure to check the previous layers before moving on to the next one.</span>"]},{"cell_type":"markdown","metadata":{"id":"31YUXtCiXEyT"},"source":["### Single Head Causal Attention (20 points)\n","\n","We will first implement the single head causal attention layer. This layer is the same as the scaled dot product attention layer but with a causal mask to prevent the model from looking into the future.\n","\n","Recall that Each head has a Key, Query and Value Matrix and the scaled dot product attention is calculated as :\n","\n","\\begin{equation}\n","\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n","\\end{equation}\n","\n","where $d_k$ is the dimension of the key matrix.\n","\n","Figure below from the original paper shows how the layer is to be implemented.\n","\n","![](./Images/Single_Head.png)\n","\n","Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"]},{"cell_type":"markdown","metadata":{"id":"AOy3ZV8jXEyT"},"source":["Please complete the `SingleHeadAttention` class in `model.py`"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":1346,"status":"ok","timestamp":1716259023071,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"u5GARYvJXEyU","outputId":"e9236f4f-55eb-461f-effd-378dd4359075"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'TEST CASE PASSED!!!'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["model = SingleHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.embed_dim//4, MiniGPTConfig.embed_dim//4) # configs are set as such for testing do not modify\n","\n","tests.check_singleheadattention(model, path_to_gpt_tester, device)"]},{"cell_type":"markdown","metadata":{"id":"Pbd9Z_x3XEyU"},"source":["### Multi Head Attention (10 points)\n","\n","Now that we have a single head working, we will now scale this across multiple heads, remember that with multihead attention we compute perform head number of parallel attention operations. We then concatenate the outputs of these parallel attention operations and project them back to the desired dimension using an output linear layer.\n","\n","Figure below from the original paper shows how the layer is to be implemented.\n","\n","![](./Images/MultiHead.png)\n","\n","Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"]},{"cell_type":"markdown","metadata":{"id":"LLKBwljcXEyU"},"source":["Please complete the `MultiHeadAttention` class in `model.py` using the `SingleHeadAttention` class implemented earlier."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":172,"status":"ok","timestamp":1716259026180,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"UfSXU11PXEyU","outputId":"32367b92-9cd9-4ca4-89bc-0206b82ba844"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'TEST CASE PASSED!!!'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["model = MultiHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n","\n","tests.check_multiheadattention(model, path_to_gpt_tester, device)"]},{"cell_type":"markdown","metadata":{"id":"UYp3TYR2XEyU"},"source":["### Feed Forward Layer (5 points)\n","\n","As discussed in lecture, the attention layer is completely linear, in order to add some non-linearity we add a feed forward layer. The feed forward layer is a simple two layer MLP with a GeLU activation in between.\n","\n","Please complete the `FeedForwardLayer` class in `model.py`"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":142,"status":"ok","timestamp":1716259028967,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"lQWLUxu8XEyU","outputId":"46c2f5ca-7caa-4d06-ae6f-12f67d3bb856"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'TEST CASE PASSED!!!'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["model = FeedForwardLayer(MiniGPTConfig.embed_dim)\n","\n","tests.check_feedforward(model, path_to_gpt_tester, device)"]},{"cell_type":"markdown","metadata":{"id":"QdPl8M3xXEyV"},"source":["### LayerNorm (10 points)\n","\n","We will now implement the layer normalization layer. Layernorm is used across the model to normalize the activations of the previous layer. Recall that the equation for layernorm is given as:\n","\n","\\begin{equation}\n","\n","\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\odot \\gamma + \\beta\n","\n","\\end{equation}\n","\n","With the learnable parameters $\\gamma$ and $\\beta$.\n","\n","Remember that unlike batchnorm we compute statistics across the feature dimension and not the batch dimension, hence we do not need to keep track of running averages."]},{"cell_type":"markdown","metadata":{"id":"BDDKqXdwXEyV"},"source":["Please complete the `LayerNorm` class in `model.py`"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":152,"status":"ok","timestamp":1716259031877,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"PpvaFMyYXEyV","outputId":"fd1d3f25-0db7-4d7b-f6fa-ecbac26dbc23"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'TEST CASE PASSED!!!'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["model = LayerNorm(MiniGPTConfig.embed_dim)\n","tests.check_layernorm(model, path_to_gpt_tester, device)"]},{"cell_type":"markdown","metadata":{"id":"E3wvMSAqXEyW"},"source":["### Transformer Layer (15 points)\n","\n","We have now implemented all the components of the transformer layer. We will now put it all together to create a transformer layer. The transformer layer consists of a multi head attention layer, a feed forward layer and two layer norm layers.\n","\n","Please use the following order for each component (Varies slightly from the original attention paper):\n","1. LayerNorm\n","2. MultiHeadAttention\n","3. LayerNorm\n","4. FeedForwardLayer\n","\n","Remember that the transformer layer also has residual connections around each sublayer.\n","\n","The below figure shows the structure of the transformer layer you are required to implement.\n","\n","![](./Images/Prenorm.png)\n","\n","Image Credit : [CogView](https://arxiv.org/pdf/2105.13290)"]},{"cell_type":"markdown","metadata":{"id":"93a8PGp8XEyW"},"source":["Implement the `TransformerLayer` class in `model.py`"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":155,"status":"ok","timestamp":1716259037999,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"evmTsPM3XEyW","outputId":"7d705ed1-dac5-4a81-eebf-227d3bfbb70c"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'TEST CASE PASSED!!!'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model =  TransformerLayer(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n","tests.check_transformer(model, path_to_gpt_tester, device)"]},{"cell_type":"markdown","metadata":{"id":"Iw2B7yWgXEyW"},"source":["### Putting it all together : MiniGPT (15 points)\n","\n","We are now ready to put all our layers together to build our own MiniGPT!\n","\n","The MiniGPT model consists of an embedding layer, a positional encoding layer and a stack of transformer layers. The output of the transformer layer is passed through a linear layer (called head) to get the final output logits. Note that in our implementation we will use [weight tying](https://arxiv.org/abs/1608.05859) between the embedding layer and the final linear layer. This allows us to save on parameters and also helps in training."]},{"cell_type":"markdown","metadata":{"id":"mlS2bjeLXEyW"},"source":["Implement the `MiniGPT` class in `model.py`"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":327,"status":"ok","timestamp":1716259041696,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"sSTzSQJKXEyX","outputId":"db07ea7a-9a59-4104-fd01-8e418fd35e69"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'TEST CASE PASSED!!!'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["model = MiniGPT(MiniGPTConfig)\n","tests.check_miniGPT(model, path_to_gpt_tester, device)"]},{"cell_type":"markdown","metadata":{"id":"U_b9vrMJXEyb"},"source":["### Attempt at training the model (5 points)\n","\n","We will now attempt to train the model on the text data. We will use the same text data as before. Please scale down the model parameters in the config file to a smaller value to make training feasible.\n","\n","Use the same training script we built for the Bigram model to train the MiniGPT model. If you implemented it correctly it should work just out of the box!\n","\n","**NOTE** : We will not be able to train the model to completion in this assignment. Unfortunately, without access to a relatively powerful GPU, training a large enough model to see good generation is not feasible. However, you should be able to see the loss decreasing over time. <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You do not need to run this for more than 5000 iterations or 1 hour of training."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"elapsed":30797,"status":"ok","timestamp":1716239140616,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"ZPjYpa1yfwhZ","outputId":"4aa6d8ce-b5a3-4382-caa3-b890ae86bd6c"},"outputs":[{"data":{"application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":413,"referenced_widgets":["92ee8976e1554543a47cb8879d61b519","c0c728f3089c44ae9c3b16b0d0334a3e","81b3694084a6482c885d8e7e707abda4","fb7fb821557f4fd39b6f15bfe148756c","22705cd267034e48aee78d0e4afe4e73","745bd46cc2c44ac9821acc426fe94f54","eb87e1cab53846c09c0e3fd7acb86929","2952ec05d69d4628badcec6497c731a2"]},"executionInfo":{"elapsed":1994574,"status":"ok","timestamp":1716241153368,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"q4x22-PqaN4a","outputId":"022aab63-97f9-4926-fdd2-4db6ab4552e7"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mawong0811\u001b[0m (\u001b[33manthonys-projects\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/23-24/239/MiniGPT/wandb/run-20240520_210600-hbsfka21</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/anthonys-projects/dl2_proj3/runs/hbsfka21' target=\"_blank\">glorious-cloud-54</a></strong> to <a href='https://wandb.ai/anthonys-projects/dl2_proj3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/anthonys-projects/dl2_proj3' target=\"_blank\">https://wandb.ai/anthonys-projects/dl2_proj3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/anthonys-projects/dl2_proj3/runs/hbsfka21' target=\"_blank\">https://wandb.ai/anthonys-projects/dl2_proj3/runs/hbsfka21</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["number of trainable parameters: 3.32M\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [32:41<00:00, 1961.91s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/1], Loss: 3.8294\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92ee8976e1554543a47cb8879d61b519","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>█▆▅▅▄▄▄▃▃▂▃▃▂▂▂▂▃▂▂▂▂▂▁▂▂▁▂▂▂▃▂▁▂▁▃▂▂▁▂▂</td></tr><tr><td>Validation loss</td><td>█▅▅▄▄▃▃▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>3.48677</td></tr><tr><td>Validation loss</td><td>3.82858</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">glorious-cloud-54</strong> at: <a href='https://wandb.ai/anthonys-projects/dl2_proj3/runs/hbsfka21' target=\"_blank\">https://wandb.ai/anthonys-projects/dl2_proj3/runs/hbsfka21</a><br/> View project at: <a href='https://wandb.ai/anthonys-projects/dl2_proj3' target=\"_blank\">https://wandb.ai/anthonys-projects/dl2_proj3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240520_210600-hbsfka21/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["\"\"\"\n","Training file for the models we implemented\n","\"\"\"\n","\n","from pathlib import Path\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.utils\n","from torch.utils.data import DataLoader\n","from einops import rearrange\n","import wandb\n","from tqdm import tqdm\n","\n","from model import BigramLanguageModel, MiniGPT\n","from dataset import TinyStoriesDataset\n","from config import BigramConfig, MiniGPTConfig\n","\n","\n","MODEL = \"minigpt\"  # bigram or minigpt\n","\n","if MODEL == \"bigram\":\n","    config = BigramConfig\n","    model = BigramLanguageModel(config)\n","elif MODEL == \"minigpt\":\n","    config = MiniGPTConfig\n","    model = MiniGPT(config)\n","else:\n","    raise ValueError(\"Invalid model name\")\n","\n","\n","# Initialize wandb if you want to use it\n","if config.to_log:\n","    wandb.init(project=\"dl2_proj3\")\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","train_dataset = TinyStoriesDataset(\n","    config.path_to_data,\n","    mode=\"train\",\n","    context_length=config.context_length,\n",")\n","eval_dataset = TinyStoriesDataset(\n","    config.path_to_data, mode=\"test\", context_length=config.context_length\n",")\n","\n","train_dataloader = DataLoader(\n","    train_dataset, batch_size=config.batch_size, pin_memory=True\n",")\n","eval_dataloader = DataLoader(\n","    eval_dataset, batch_size=config.batch_size, pin_memory=True\n",")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(\"number of trainable parameters: %.2fM\" % (count_parameters(model) / 1e6,))\n","\n","\n","if not Path.exists(config.save_path):\n","    Path.mkdir(MiniGPTConfig.save_path, parents=True, exist_ok=True)\n","\n","\n","### ==================== START OF YOUR CODE ==================== ###\n","\"\"\"\n","You are required to implement the training loop for the model.\n","\n","Please keep the following in mind:\n","- You will need to define an appropriate loss function for the model.\n","- You will need to define an optimizer for the model.\n","- You are required to log the loss (either on wandb or any other logger you prefer) every `config.log_interval` iterations.\n","- It is recommended that you save the model weights every `config.save_iterations` iterations you can also just save the model with the best training loss.\n","\n","Please check the config file to see the different configurations you can set for the model.\n","NOTE :\n","The MiniGPT config has params that you do not need to use, these were added to scale the model but are\n","not a required part of the assignment.\n","Feel free to experiment with the parameters and I would be happy to talk to you about them if interested :)\n","\"\"\"\n","\n","#========Set Loss Function and Optimizer========#\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 1e-4)\n","#===============================================#\n","\n","#========Set Save Path========#\n","best_model_params_path = \"./models/\" + MODEL + \"/best_model_params.pt\"\n","torch.save(model.state_dict(), best_model_params_path)\n","#=============================#\n","\n","#========Bookkeeping========#\n","best_train_loss = 1000.0\n","iteration = 0\n","num_epochs = 1\n","#===========================#\n","\n","model = model.to(device)\n","\n","#========Training Loop========#\n","for epoch_idx in tqdm(range(num_epochs)):\n","    for inputs, targets in train_dataloader:\n","        if iteration==5000: #hard code to stop at 5000 iterations\n","          break\n","        model.train()\n","        optimizer.zero_grad()\n","        inputs.to(device)\n","        targets.to(device)\n","        logits = model(inputs)\n","        logits = logits.transpose(1,2)\n","        loss = criterion(logits, targets)\n","        loss.backward()\n","        optimizer.step()\n","        iteration += 1\n","\n","        if loss.item() < best_train_loss: #save model with best training loss\n","            best_train_loss = loss.item()\n","            torch.save(model.state_dict(), best_model_params_path)\n","\n","        if iteration%config.log_interval == 0: #record the loss in wandb and validate\n","            wandb.log({\"Training loss\": loss.item()})\n","            model.eval()\n","            with torch.no_grad():\n","                total_loss = 0.0\n","                num_batches = 20\n","                for i in range(num_batches): #validate on 20 batches from the eval dataset\n","                    batch = next(iter(eval_dataloader))\n","                    inputs, targets = batch\n","                    inputs.to(device)\n","                    targets.to(device)\n","                    logits = model(inputs)\n","                    logits = logits.transpose(1,2)\n","                    loss = criterion(logits, targets)\n","                    total_loss += loss.item()\n","                val_loss = total_loss/num_batches\n","                wandb.log({\"Validation loss\": val_loss})\n","\n","    print(f'Epoch [{epoch_idx+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","wandb.finish()\n","#=============================#\n"]},{"cell_type":"markdown","metadata":{"id":"a61_yPUeXEyb"},"source":["### Train and Valid Plots\n","*Note model was trained only for 5000 iterations even though it looks like it trained for one epoch.\n","\n","** Show the training and validation loss plots **\n","![image](./Images/minigptgraphs.png)"]},{"cell_type":"markdown","metadata":{"id":"XZzVo2MHXEyb"},"source":["### Generation (5 points)\n","\n","\n","Perform generation with the model that you trained. Copy over the generation function you used for the Bigram model not the `miniGPT` class and generate a mini story using the same seed sentence.\n","\n","    `\"once upon a time\"`"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1666,"status":"ok","timestamp":1716243875226,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"NF4eTaKbrmMk","outputId":"c6ecff7f-250a-4736-f4be-16a350c2e5cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating text starting with: torch.Size([4])\n","Once upon a time, there was a little boy named Timmy. Lily learned that what not lost me. I love you, so sweetie came for you.Once upon a time, there was a little boy named Lily. She loved her hand,nect wanted to get up so good. She had a big cat named Tim. She loved towat out a toy of his friend, laughing Timmy. She fell away, but help. They flew down and thanked the hospital.Once upon a long boy named Lily. She loved to play on the cozy icebles.\n","One day, a little boy went under the water every day. One day, he saw a chocolate bit old little girl named Timmy. One day, her mom said, \"Hi, you am moment, Momo was not not for elsewhere. Tim held it from front. A white rock that clap sw THESEie would make that warm and showed it. Suddenly, Timmy a big fish named Timmy. She loved to look\n"]}],"source":["model = MiniGPT(MiniGPTConfig)\n","best_model_params_path = \"./models/minigpt/best_model_params.pt\"\n","model.load_state_dict(torch.load(best_model_params_path))\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","gen_sent = \"Once upon a time\"\n","gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n","print(\"Generating text starting with:\", gen_tokens.shape)\n","gen_tokens = gen_tokens.to(device)\n","model.eval()\n","print(\n","    tokenizer.decode(\n","        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n","    )\n",")"]},{"cell_type":"markdown","metadata":{"id":"3XtPAYZoXEyc"},"source":["Please answer the following questions.\n","\n","1. What can we say about the generated text in terms of grammar and coherence?\n","2. If the model is scaled with more parameters do you expect the GPT model to get substantially better? Why or why not?"]},{"cell_type":"markdown","metadata":{"id":"i7AY_P0Q_4Li"},"source":["1. The generated text can form sentences that have some semblance of grammar and coherence. Even though the model was only trained on 5000 iterations, the output is already much better than the Bigram output and looks like something a child might write. Still, there is a ways to go before it can be considered close to human language.\n","2. If the model is scaled with more parameters, the GPT model would have a higher capacity. It could get substantially better, but it would need a lot more computational resources."]},{"cell_type":"markdown","metadata":{"id":"cueNiimlXEyc"},"source":["### Scaling up the model (5 points)\n","\n","To show that scale indeed will help the model learn we have trained a scaled up version of the model you just implemented. We will load the weights of this model and generate a mini story using the same seed sentence. Note that if you have implemented the model correctly just scaling the parameters and adding a few bells and whistles to the training script will results in a model like the one we will load now."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MgD0qxyXEyc"},"outputs":[],"source":["from model import MiniGPT\n","from config import MiniGPTConfig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"igKyRIDMXEyc"},"outputs":[],"source":["path_to_trained_model = \"pretrained_models/best_train_loss_checkpoint.pth\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRnsISMsXEyc"},"outputs":[],"source":["ckpt = torch.load(path_to_trained_model, map_location=device) # remove map location if using GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w1G_i_YdXEyc"},"outputs":[],"source":["# Set the configs for scaled model\n","MiniGPTConfig.context_length = 512\n","MiniGPTConfig.embed_dim = 256\n","MiniGPTConfig.num_heads = 16\n","MiniGPTConfig.num_layers = 8"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1387,"status":"ok","timestamp":1716188220833,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"Cu8LHtleXEyc","outputId":"92fcae0a-dc1d-4dd8-9241-b29af575ecff"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["# Load model from checkpoint\n","model = MiniGPT(MiniGPTConfig)\n","model.load_state_dict(ckpt[\"model_state_dict\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_yd9xiBXEyd"},"outputs":[],"source":["tokenizer = tiktoken.get_encoding(\"gpt2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15199,"status":"ok","timestamp":1716188460295,"user":{"displayName":"ANTHONY WONG","userId":"09225024121631730650"},"user_tz":420},"id":"crL43ZBqXEyd","outputId":"afa4074a-3a45-4996-d5cf-a69a1470f058"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating text starting with: torch.Size([4])\n","Once upon a time, there was a woman who lived in a big house. She was very charming and loved to do her chores. One day, her mom said they were going to a party. They said it was an important dance party with their friends, especially the ones with their best friends, and how much fun they had played together.Once upon a time, in a small house, there lived a little girl named Lily. One day, she went to school with Max. Max was a tall giraffe with big, long neck. One day, Lily went../. polka pets were playing in nature, chasing butterflies and looking for flowers. She learned that sometimes, things don't go as planned, but with your help, we can learn from each other. Thank you for being nice. I wish I could give both each other hugs.\" Sam thought about it and said, \"Yes, please.\"\n","In Mutual's story, der and della stayed in discussed danger and helped the captain. She stopped\n"]}],"source":["model.to(device)\n","gen_sent = \"Once upon a time\"\n","gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n","print(\"Generating text starting with:\", gen_tokens.shape)\n","gen_tokens = gen_tokens.to(device)\n","model.eval()\n","print(\n","    tokenizer.decode(\n","        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n","    )\n",")"]},{"cell_type":"markdown","metadata":{"id":"y9ZAR9VJXEyd"},"source":["## Bonus (5 points)\n","\n","The following are some open ended questions that you can attempt if you have time. Feel free to propose your own as well if you have an interesting idea.\n","\n","1. The model we have implemented is a decoder only model. Can you implement the encoder part as well? This should not be too hard to do since most of the layers are already implemented.\n","2. What are some improvements we can add to the training script to make training more efficient and faster? Can you concretely show that the improvements you made help in training the model better?\n","3. Can you implement a beam search decoder to generate the text instead of greedy decoding? Does this help in generating better text?\n","4. Can you further optimize the model architecture? For example, can you implement [Multi Query Attention](https://arxiv.org/abs/1911.02150) or [Grouped Query Attention](https://arxiv.org/pdf/2305.13245) to improve the model performance?"]},{"cell_type":"markdown","metadata":{"id":"g2M_1Baq9dKH"},"source":["I did bonus question #1. The encoder is at the very bottom of the model.py file. There are two classes: an encoder block class along with an encoder class. The encoder block follows the traditional encoder architecture with a multihead attention layer, a residual connection and layernorm layer, a feedforward layer, and another residual connection/layernorm layer. The encoder class consists of a vocab embedding layer, a positional encoding of the context that gets added to the vocab embedding and stacked encoder blocks. Just like the MiniGPT model, the user can specify model parameters in the config file such as the number of stacked encoder blocks, vocab size, context size, number of attention heads, and the embedding dimension."]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"148df20632314c249847b1ec7b4c8063":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1e141c14933e47aa83eca5e621ec9f98":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8681b98505f2479484ae4a059e264066","placeholder":"​","style":"IPY_MODEL_69e46241661246fda6b031511c4c74b8","value":"0.011 MB of 0.011 MB uploaded\r"}},"22705cd267034e48aee78d0e4afe4e73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2952ec05d69d4628badcec6497c731a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"69e46241661246fda6b031511c4c74b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"745bd46cc2c44ac9821acc426fe94f54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78850579aa4c41d68f9b4f40c9153992":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81b3694084a6482c885d8e7e707abda4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb87e1cab53846c09c0e3fd7acb86929","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2952ec05d69d4628badcec6497c731a2","value":1}},"86569f1aa7534d3abca46863951fd396":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_1e141c14933e47aa83eca5e621ec9f98","IPY_MODEL_c253b971986546c0b1f89e7246d611a4"],"layout":"IPY_MODEL_f0989837b1d4404ba04095048cd0c351"}},"8681b98505f2479484ae4a059e264066":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92ee8976e1554543a47cb8879d61b519":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_c0c728f3089c44ae9c3b16b0d0334a3e","IPY_MODEL_81b3694084a6482c885d8e7e707abda4"],"layout":"IPY_MODEL_fb7fb821557f4fd39b6f15bfe148756c"}},"c0c728f3089c44ae9c3b16b0d0334a3e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22705cd267034e48aee78d0e4afe4e73","placeholder":"​","style":"IPY_MODEL_745bd46cc2c44ac9821acc426fe94f54","value":"0.011 MB of 0.011 MB uploaded\r"}},"c253b971986546c0b1f89e7246d611a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_78850579aa4c41d68f9b4f40c9153992","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_148df20632314c249847b1ec7b4c8063","value":1}},"eb87e1cab53846c09c0e3fd7acb86929":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0989837b1d4404ba04095048cd0c351":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb7fb821557f4fd39b6f15bfe148756c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
